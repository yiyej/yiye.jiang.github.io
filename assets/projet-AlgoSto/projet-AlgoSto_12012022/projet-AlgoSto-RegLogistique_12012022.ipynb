{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://uf-mi.u-bordeaux.fr/MSS/\" ><img src=\"https://www.math.u-bordeaux.fr/~jbigot/Site/Enseignement_files/logo_MAS_MSS.jpg\" style=\"float:left; max-width: 400px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"https://www.math.u-bordeaux.fr/\" ><img src=\"https://www.math.u-bordeaux.fr/~jbigot/Site/Enseignement_files/LogoIMB.jpg\" style=\"float:right; max-width: 250px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 200px; display: inline\" alt=\"Python\"/></a> UE M2 Master MAS-MSS et CIMI ISI Projet Données Massives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmes stochastiques pour la régression logistique avec <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 150px; display: inline\" alt=\"Python\"/></a> & <a href=\"http://scikit-learn.org/stable/#\"><img src=\"http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\" style=\"max-width: 180px; display: inline\" alt=\"Scikit-Learn\"/></a>\n",
    "**Résumé**: \n",
    "\n",
    "Dans ce projet, vous devrez implémenter de trois algorithmes stochastiques pour la résolution d'un problème de régression logistique dans le cardre de classification supervisée binaire (à $K=2$ classes): \n",
    "- un algorithme de descente de gradient stochastique usuel tel que vous l'avez vu en cours\n",
    "- l'algorithme ADAM : https://arxiv.org/pdf/1412.6980.pdf\n",
    "- l'algorithme de Newton stochastique : https://arxiv.org/abs/1904.07908\n",
    "\n",
    "Pour évaluer ces algorithmes, vous devrez les appliquer à trois problèmes de classification binaire différents:\n",
    "- Des données 2D simulées\n",
    "- La classification des images MNIST de taille 28 * 28 ( *liste de tâches à donner le 17 Janvier*)\n",
    "- La prédiction de présence d'éoliennes dans des images de taille 128 * 128 * 3 ( *liste de tâches à donner le 17 Janvier*)\n",
    "\n",
    "et plus comaprer leurs performances en terme de précision de la classification ainsi que de temps de convergence. \n",
    "\n",
    "Pour le rapport: \n",
    "\n",
    "- à réaliser en jupyter notebook, avec le code et **la sortie**\n",
    "- à me rendre **avant 7 Février**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pré-histoire...**\n",
    "\n",
    "``fit_intercept`` :) \n",
    "\n",
    "``intercept_scaling`` :(\n",
    "\n",
    "https://stackoverflow.com/questions/48202410/confusion-in-sklearn-logistic-regression\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Algorithme stochastique ADAM (Adaptive Moment Estimation)\n",
    "\n",
    "Les limitations du SGD? Un pas $\\gamma_k$ partagé par tous les composantes:\n",
    "\n",
    "- un trop grand pas -> paramètre va franchir l'optimiseur \n",
    "\n",
    "- un trop petit pas -> descendre lentement \n",
    "\n",
    "Si certains composantes possèdent les grands gradients mais en même temps certain d'autres possèdent les petits -> difficulté en la réglage de pas, surtout quand $d$ est grand "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En route vers ADAM\n",
    "\n",
    "**AdaGrad**\n",
    "\n",
    "L'algo adaptatif réduit la taille des pas sur les composantes où le gradient est grand et laisse les pas grand sur les autres composantes:\n",
    "\n",
    "$$\\theta_{k+1} = \\theta_k -\\gamma \\frac{\\nabla f(Z_{k+1}, \\theta_k)}{\\sqrt{\\sum_{i \\leqslant k} \\nabla f(Z_i,\\theta_{i-1})^2}+\\varepsilon}$$\n",
    "avec $\\varepsilon$ petit et où il faut comprendre la racine carrée, le carré et la division, composante par composante. La constante de normalisation est déterminée par la moyenne du gradient carré. \n",
    "\n",
    "**Root Mean Square Propagation (RMSProp)**\n",
    "\n",
    "Le problème est que tout l'historique des gradient intervient dans la pondération avec le même poids. On peut modifier l'algorithme en ajoutant une moyenne mobile. On obtient :\n",
    "$$\\begin{cases}\n",
    "v_{k+1} = \\beta v_k + (1-\\beta) \\nabla f(Z_{k+1}, \\theta_k)^2\\\\\n",
    "\\theta_{k+1} = \\theta_k - \\gamma \\frac{\\nabla f(Z_{k+1}, \\theta_k)}{\\sqrt{v_{k+1}}+\\varepsilon}\n",
    "\\end{cases}$$\n",
    "\n",
    "<img src=\"sgd_rmsprop.png\" width=400 height=400/>\n",
    "(Img src: talk << Convergence and Dynamical Behavior of the ADAM Algorithm for Non Convex Stochastic Optimization>> - Anas Barakat )\n",
    "\n",
    "**Inertie**\n",
    "\n",
    "De manière similaire, on peut adopter la moyenne mobile de tout l'historique des gradient pour avoir une meilleure estimation du gradient de $\\mathcal{L}$, en espérant lisser la trajectoire vers l'optimum, afin d'accélérer la convergence.\n",
    "\n",
    "$$\\begin{cases}\n",
    "m_{k+1} = \\alpha m_k + (1-\\alpha) \\nabla f(Z_{k+1}, x_k)\\\\\n",
    "\\theta_{k+1} = \\theta_k - \\gamma m_{k+1}\n",
    "\\end{cases}$$\n",
    "\n",
    "<img src=\"inertie.png\" width=400 height=400/>\n",
    "(Img src: talk << Convergence and Dynamical Behavior of the ADAM Algorithm for Non Convex Stochastic Optimization>> - Anas Barakat )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***RMSProp + Inertie = ADAM***\n",
    "\n",
    "On décrit ici l'algorithme ADAM (papier fondateur: https://arxiv.org/abs/1412.6980).\n",
    "\n",
    "*Paramètres de l'algorithme* :\n",
    "\n",
    "- $\\gamma > 0$ : learning rate / taille du pas\n",
    "\n",
    "- $\\alpha$ et $\\beta$ paramètres dans $[0,1)$ pour les estimées de moments : choix recommandés $\\alpha = 0.9$ et $\\beta = 0.999$\n",
    "- $\\varepsilon = 10^{-7}$ ou $\\varepsilon = 10^{-8}$: éviter l'explosion lors de presque zero $\\hat{v}_k$\n",
    "\n",
    "*Initialisation* : choix de $\\hat{\\theta}^{(0)} \\in \\mathbb{R}^{d+1}$\n",
    "\n",
    "- $m_0 = 0$\n",
    "- $v_0 = 0$\n",
    "\n",
    "\n",
    "*Répéter pour $1 \\leq k \\leq n$*\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "g_k & = & \\nabla f(X_k,Y_k,\\hat{\\theta}^{(k-1)}) \\\\\n",
    "m_k & = & \\alpha m_{k-1} + (1-\\alpha) g_k \\\\\n",
    "v_k & = & \\beta v_{k-1} + (1-\\beta) g_{k}^2 \\\\\n",
    "\\hat{m}_k & = & \\frac{m_k}{1-\\alpha^k} \\\\\n",
    "\\hat{v}_k & = & \\frac{v_k}{1-\\beta^k} \\\\\n",
    "\\hat{\\theta}^{(k)} & = & \\hat{\\theta}^{(k-1)} - \\gamma \\frac{\\hat{m}_k}{\\sqrt{\\hat{v}_k} + \\varepsilon}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "$^*$ Comme $m$ et $v$ sont initialisés comme 0, les auteurs d'Adam observent qu'ils sont biaisés vers zéro, en particulier pendant les premiers itérations, et surtout lorsque les pas $\\alpha$ et $\\beta$ sont faibles. Ils renormalisent $m_k$ (resp. $v_k$) par $1-\\alpha^k$ (resp. $1-\\beta^k$) afin d'avoir les plus grandes éstimées $\\hat{m}_k$ et $\\hat{v}_k$.\n",
    "\n",
    "**Version Mini-batch**\n",
    "\n",
    "*Initialisation* : choix de $\\hat{\\theta}^{(0)} \\in \\mathbb{R}^{d+1}$, de taille du batch $M$ et du nombre d'itérations à effectuer $n\\_ epoch$\n",
    "\n",
    "*Répéter pour $1 \\leq t \\leq n\\_ epoch$*\n",
    "\n",
    "*Permutation au harsard de l'ensemble d'apprentissage, dont nouveau ordre noté par $\\underbrace{(X_{t_1}, Y_{t_1})}_{Z_{t_1}}, \\ldots, \\underbrace{(X_{t_n}, Y_{t_n})}_{Z_{t_n}}$*\n",
    "\n",
    "*Former les mini-batchs de nouveau*\n",
    "$$\n",
    "\\underbrace{Z_{t_1},\\ldots,Z_{t_M}}_{B_1 = \\{t_1,\\ldots,t_M\\}}, \\quad \\underbrace{Z_{t_{M+1}},\\ldots,Z_{t_{2M}}}_{B_2 = \\{t_M,\\ldots,t_{2M}\\}}, \\quad\\underbrace{Z_{t_{2M+1}},\\ldots,Z_{t_{3M}}}_{B_3 = \\{t_{2M+1},\\ldots,t_{3M}\\}}, \\quad \\ldots\n",
    "$$\n",
    "\n",
    "*Répéter pour $1 \\leq l \\leq \\frac{n}{M}$*\n",
    "\n",
    "**$k = (t-1)*\\frac{n}{M} + l$**\n",
    "\n",
    "*Remplacer la 1ère formule dans la version stochastique avec*\n",
    "\n",
    "$$\n",
    "g_k = \\frac{1}{M} \\sum_{i \\in B_{l}} \\nabla f(X_{i}, Y_{i}, \\hat{\\theta}^{(k-1)})\n",
    "$$\n",
    "\n",
    "Les reste formules sont maintenues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Algorithme stochastique de Newton\n",
    "\n",
    "Trade-off: \n",
    "\n",
    "- plus du fameux *Hyperparameter tuning*\n",
    "\n",
    "- la matrice inverse supplementaire \n",
    "\n",
    "**Rappel de l'algorithme de Newton déterministe :**\n",
    "\n",
    "L'algorithme de Newton permet de trouver un zéro d'une fonction $h$. L'idée est que à chaque itération, on approche la fonction à minimiser par une droite dans un voisinage de $x_k$, et prends le zéro de la droite comme l'approximation de zero de $h$, donc aussi $x_{k+1}$. \n",
    "\n",
    "<img src=\"Newton_iteration.png\" width=400 height=400/>\n",
    "(Img src: wiki)\n",
    "\n",
    "En dimension $1$ on a :\n",
    "- On initialise à $x_0$\n",
    "- On appelle $x_1$ l'intersection de la droite $y = h'(x_0)(x-x_0)+h(x_0)$ avec $0$. On trouve\n",
    "$$x_1=x_0 - \\frac{h(x_0)}{h'(x_0)}$$\n",
    "- On itère.\n",
    "\n",
    "c'est-à-dire \n",
    "$$x_{k+1} = x_k- \\frac{h(x_k)}{h'(x_k)}$$\n",
    "\n",
    "Ici, on cherche un zéro de la dérivée $f'$ (en dimension 1) ou du gradient $\\nabla f$ (en dimension plus grande). \n",
    "Cela donne, pour $h=f'$\n",
    "$$x_{k+1} = x_k- \\frac{f'(x_k)}{f''(x_k)}$$\n",
    "\n",
    "En dimension plus grande que $1$, on obtient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_{k+1} = x_k- (\\nabla^2 f(x_k))^{-1} \\nabla f(x_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un algorithme stochastique de Newton proposé par https://arxiv.org/abs/1904.07908 \n",
    "\n",
    "Au lieu de calculer $\\nabla^2 f(X_i, Y_i,\\hat{\\theta}_k)$ et l'inverser systématiquement, il propose d'utiliser \n",
    "$S_k = \\sum\\limits_{i=1}^k\\nabla^2 f(X_i, Y_i,\\hat{\\theta}_{k-1})$ dans chaque itération, et l'algo est le schéma itératif suivant :\n",
    "\n",
    "*Initialisation* : choix de $\\hat{\\theta}^{(0)} \\in \\mathbb{R}^{d+1}$ et $n\\_ epoch$\n",
    "\n",
    "*Répéter pour $1 \\leq t \\leq n\\_ epoch$*\n",
    "\n",
    "Permutation des données\n",
    "\n",
    "*Répéter pour $1 \\leq i \\leq n$*\n",
    "\n",
    "$k = ...$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}^{(k)} = \\hat{\\theta}^{(k-1)} - S_{k}^{-1} \\nabla f(X_{t_i},Y_{t_i},\\hat{\\theta}^{(k-1)})\n",
    "$$\n",
    "\n",
    "où on calcule une version récursive de l'inverse de la matrice Hessienne stochastique $S_{k}$ définie par\n",
    "\n",
    "$$\n",
    "S_{0} = I \\mbox{ et } S_{k} = S_{k-1} + \\frac{1}{a_{k}\\big(\\hat{\\theta}^{(k-1)}\\big)} X_{t_i} X_{t_i}^T,  \\mbox{ avec } a_{k}\\big(\\hat{\\theta}^{(k-1)}\\big) = \\big(\\exp(-\\langle \\hat{\\theta}^{(k-1)} , X_{t_i} \\rangle /2)+\\exp(\\langle \\hat{\\theta}^{(k-1)} , X_{t_i} \\rangle /2)\\big)^2\n",
    "$$\n",
    "\n",
    "grâce à la formule de Sherman–Morrison–Woodbury https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula qui donne que\n",
    "\n",
    "$$\n",
    "S_{k}^{-1} = S_{k-1}^{-1} - \\frac{1}{a_{k}\\big(\\hat{\\theta}^{(k-1)}\\big) + X_{t_i}^T S_{k-1}^{-1}X_{t_i}} S_{k-1}^{-1} X_{t_i} X_{t_i}^T S_{k-1}^{-1}\n",
    "$$\n",
    "\n",
    "Ici l'algorithme stochastique de Newton est défini par des Batch de taille 1 uniquement ! Il est possible d'avoir une version avec des batch de taille plus grande mais ceci complique le coût calculatoire de la mise à jour de l'inversion de la matrice Hessienne stochastique $S_{k}$. (Formule woodbury demande la difference du rang 1)\n",
    "\n",
    "Notons que cet algorithme ne demande pas la calibration de paramètres !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travail à réaliser\n",
    "\n",
    "Implémentez l'algorithme stochastique de Newton et le mini-batch ADAM sur l'ensemble d'appretissage et faites varier les hyper-paramètres du dernier, e.g. $\\mbox{n_epoch} = 20, \\mbox{batch_size} = 10, \\gamma = 0.01$\n",
    "\n",
    "$^*$ ! ***ne vous embêtez pas par la réglage des hyper-paramètres, l'important est d'observer leurs influences sur la performance de l'algo***\n",
    "\n",
    "Pour tous les trois algorithmes stochastiques, i.e. \n",
    "- le mini-batch GD avec la meilleure $^*$ combinaison de hyperparamètres\n",
    "- le mini-batch ADAM avec la meilleure $^*$ combinaison de hyperparamètres\n",
    "- l'algorithme stochastique de Newton avec de suffisant epochs\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "1. tracez la fontière obtenue et comparez les 3 fontières avec la fontière de la régression logistique \n",
    "\n",
    "2. tracez les indicateurs de performance, plus exactement \n",
    "\n",
    " - relevez le temps d'exécution de chaque itération\n",
    " \n",
    " A la fin de chaque itération $k$, avec le dernier $\\hat{\\theta}^{(k)}$\n",
    " - predites les classes des tous echantillons dans l'ensemble d'appretissage et culculez le taux d'erreur train\n",
    " - predites les classes des tous echantillons dans l'ensemble test et culculez le taux d'erreur test \n",
    " - calculez la loss $\\mathcal{L}_n$ i.e. l'opposé de log-vraisemblance normalisé train (utiliser tous les echantillons dans l'ensemble d'appretissage)\n",
    " - calculez la loss $\\mathcal{L}_n$ i.e. l'opposé de log-vraisemblance normalisé test (utiliser tous les echantillons dans l'ensemble test)\n",
    "\n",
    " tracez ainsi: \n",
    " - l'évolution du taux d'erreur train / test en fonction des itérations\n",
    " - l'évolution du taux d'erreur train / test en fonction du temps $^*$  \n",
    " - l'évolution du train / test loss en fonction des itérations \n",
    " - l'évolution du train / test loss en fonction du temps\n",
    " \n",
    "$^*$: *ctd l'évolution du taux d'erreur train de l'itération $k$, en fonction du temps cumulatif pour tous les $k$ itérations déja exécutées, k = 1, 2, ...*\n",
    " \n",
    " comparez et commentez les performances de 3 algorithmes \n",
    " \n",
    "**Question**\n",
    "\n",
    "Pour le mini-batch GD et le mini-batch ADAM :\n",
    "\n",
    "Que pensez-vous de l'influence du choix du pas $\\gamma_k$ ou $\\gamma$ sur le temps de la convergence de l'algorithme ainsi que sur la performance de modèle éstimé, autrement dit, sur le theta inféré par rapport au theta inféré par ``lr.fit(X, y)``? Que pensez-vous de l'influence du choix de la taille du batch ?"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
